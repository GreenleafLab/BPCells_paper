# arrayjob helpers

The idea of this is to have helpers for splitting up repetitive jobs efficiently with
support for re-running just failed jobs. This helps to avoid duplicated work and avoid
manual effort tracking which jobs failed.

Most of these benchmarks were run on a SLURM cluster, though the `run.py` script also supports
running on the local machine which was used for laptop benchmarking.

## How to re-run a benchmark

Let's say we want to re-run the tasks from `compression/rna-1M-cell`. The steps are:

0. Edit the [`config_vars.sh`](../config_vars.sh) file according to the comments there
1. Generate a `tasks.txt` file: `python compression/rna-1M-cell/gen_tasks.py`
    - If planning to run on slurm, you will likely want to modify the job submission parameters in `gen_tasks.py`
      to match the partition names and/or hardware constraints appropriate for your cluster.
2. Run the benchmark tasks:
    - Via slurm: `python arrayjob/run.py compression/rna-1M-cell`
    - On local computer: `python arrayjob/run.py compression/rna-1M-cell -r local`
3. After everything has run, check that all jobs finished successfully:
    - `python arrayjob/run.py compression/rna-1M-cell -d`
    - If there are errors, check outputs in `compression/rna-1M-cell/logs/error`
4. Collect per-job output files into final data tables: `Rscript compression/rna-1M-cell/collect_results.R`

Note that some benchmarks that require high memory usage like `rna-timing/pca-benchmark` generate two sets
of task files with differing resource requests. This requires calling `arrayjob/run.py` twice, once on
`rna-timing/pca-benchmark` and once on `rna-timing/pca-benchmark/high_mem_tasks`. Check the `gen_tasks.py` script to see if it makes more than one `tasks.txt` file to cover these cases. 

## System:
Input:
- Task file:
    - Named `tasks.txt`, in a folder where tracking info should go
    - First line specifies all the extra sbatch parameters, except for array and log file arguments
    - Each subsequent line should be executed as a task in the array
    - This file is usually generated by a python script `gen_tasks.py` in the same folder

File Organization:
- All files based in the same folder as `tasks.txt`
- Tasks are run with this folder as the working directory
- `completions`: One file per completed task, holding the start and end times for the task
- `logs/error`: "{task_index}_{job_id}.log" files for each of the most recent jobs with errors
- `logs/success`: "{task_index}_{job_id}.log" files for each of the most recent jobs with success
- `logs/running`: Log outputs for ongoing jobs
- `logs/archive/[error,success]`: Log outputs for repeated job runs

## Tools:
`run.py task_folder/`:
1. Checks for completions and existing files in `logs/running`, and uses that to categorize them
    into success and error
2. Submits any jobs which haven't succeeded yet.


## run.py docs
```
usage: run.py [-h] [-d] [-u] [-t] [-r {slurm,local}] task_folder

Run array jobs via sbatch

positional arguments:
  task_folder           Folder containing tasks.txt

options:
  -h, --help            show this help message and exit
  -d, --dry-run         Process logs, but don't submit new jobs
  -u, --show-unfinished
                        Print the task commands for unfinished jobs
  -t, --test            Only run the first 3 tasks as a test
  -r {slurm,local}, --runner {slurm,local}
                        Job runner
```